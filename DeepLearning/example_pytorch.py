# -*- coding: utf-8 -*-
"""Example-Pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xYtsaGJuky4gqpa32LU5JGY4Qek38xdt

# 1. Import Library
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

!pip install torch

"""# 2. Load dataset using pandas

## About dataset: Heart Disease Dataset

*   age
*   sex
*   chest pain type (4 values)
*   resting blood pressure
*   serum cholestoral in mg/dl
*   fasting blood sugar > 120 mg/dl
*   resting electrocardiographic results (values 0,1,2)
*   maximum heart rate achieved
*   exercise induced angina
*   oldpeak = ST depression induced by exercise relative to rest
*   the slope of the peak exercise ST segment
*   number of major vessels (0-3) colored by flourosopy
*   thal: 0 = normal; 1 = fixed defect; 2 = reversable defect
"""

df = pd.read_csv('heart.csv')
df.tail(15)

target_counts = df['target'].value_counts()

plt.figure(figsize=(8, 8))
plt.pie(target_counts, labels=target_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Target')
plt.axis('equal')
plt.show()

"""#3. Processing data"""

# convert data to numpy array
X = df.iloc[:, 0:-1].values
y = list(df.iloc[:, -1])

# Standardize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert the data to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long).unsqueeze(1)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.long).unsqueeze(1)

"""#4. Create a custom dataset"""

class Dataset(Dataset):
    def __init__(self, x, y):
        self.x = x
        self.y = y

    # get item for each index
    def __getitem__(self, index):
        return self.x[index], self.y[index]

    # get len of dataset
    def __len__(self):
        return len(self.x)

train_ds = Dataset(X_train_tensor, y_train_tensor)
test_ds = Dataset(X_test_tensor, y_test_tensor)

# Load data to dataloader for batch processing
train_loader = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32, shuffle=True)

"""#5. Building a Neural Network"""

# Define neural network
class Model(nn.Module):
    def __init__(self, input_features, output_features):
        super(Model, self).__init__()
        self.fc1 = nn.Linear(input_features, 5)
        self.fc2 = nn.Linear(5, 4)
        self.fc3 = nn.Linear(4, 3)
        self.fc4 = nn.Linear(3, output_features)
        self.sigmod = nn.Sigmoid()
        self.tanh = nn.Tanh()

    def forward(self, x):
        out = self.fc1(x)
        out = self.tanh(out)
        out = self.fc2(out)
        out = self.tanh(out)
        out = self.fc3(out)
        out = self.tanh(out)
        out = self.fc4(out)
        out = self.sigmod(out)
        return out

# Create neural network object
net = Model(input_features = 13, output_features = 1)

# print network architecture
print(net)

def checkpoint(model, filename):
    torch.save(model.state_dict(), filename)

# Define binary class entropy (in and output should have same shape)
criterion = torch.nn.BCELoss(size_average=True)

# Define optimizers
optimizers = {
    "SGD": torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-5),
    "Adam": torch.optim.Adam(net.parameters(), lr=0.1, weight_decay=1e-5),
    "RMSprop": torch.optim.RMSprop(net.parameters(), lr=0.1, weight_decay=1e-5),
    "AdamW": torch.optim.AdamW(net.parameters(), lr=0.1, weight_decay=1e-5),
}

epochs = 50
early_stop_thresh = 5
best_accuracy = -1
best_epoch = -1
accuracies = {optimizer_name: [] for optimizer_name in optimizers}

"""#6. Train network"""

# Iterate over different optimization algorithms
for optimizer_name, optimizer in optimizers.items():
    # Iterate over the specified number of epochs
    for epoch in range(epochs):
        # Set the network to training mode
        net.train()
        for inputs, labels in train_loader:
            inputs = inputs.float()
            labels = labels.float()
            outputs = net(inputs) # Forward prop
            loss = criterion(outputs, labels) # Loss calculation
            optimizer.zero_grad() # Clear the gradient buffer
            loss.backward() # Backprop
            optimizer.step() # Update weights

        # Set the network to evaluation mode
        net.eval()
        correct = 0
        total = 0
        # Disable gradient calculation
        with torch.no_grad():
            # Evaluate the trained model on the test dataset
            for inputs, labels in test_loader:
                inputs = inputs.float()
                labels = labels.float()
                outputs = net(inputs)

                # Convert outputs to binary predictions
                predicted = (outputs > 0.5).float()

                # Calculate total and correct predictions
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

         # Calculate accuracy
        accuracy = 100 * correct / total
        # Store accuracy for the current optimizer
        accuracies[optimizer_name].append(accuracy)

        # Print statistics
        print('Optimizer: {}, Epoch {}/{}, Accuracy: {:.2f}'.format(
            optimizer_name, epoch + 1, epochs, accuracy))

        # Check for best accuracy and early stopping
        if accuracy > best_accuracy:
            # Update best accuracy and epoch
            best_accuracy = accuracy
            best_epoch = epoch

            # Save the model checkpoint
            checkpoint(net, f"best_epoch_{optimizer_name}.pth")

        # Stop training process if the accuracy doesn't improve
        if epoch - best_epoch > early_stop_thresh:
            print(f'{optimizer_name} algorithms stopped training at epoch %d' % epoch)
            print('\n\n')
            break

"""# Comparison of optimizer algorithms"""

# Accuracy Plot
plt.figure(figsize=(10, 6))
for optimizer_name, accuracy_list in accuracies.items():
    plt.plot(range(1, len(accuracy_list) + 1), accuracy_list, label=optimizer_name)

plt.title('Accuracy vs. Epoch for Different Optimizers')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.grid(True)
plt.show()

