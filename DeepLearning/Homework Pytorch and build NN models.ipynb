{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "52100674 - TRẦN THỊ VẸN"
      ],
      "metadata": {
        "id": "jkBMunMUo8Ew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Xây dựng ví dụ các mô hình Neural Networks sử dụng Pytorch, trong đó có xử lý phần Overfitting và sử dụng Optimizers khác nhau.\n",
        "\n",
        "2) Hãy trình bày về Pytorch:\n",
        "\n",
        "- Những đặc điểm chính khi so sánh với tensorflow\n",
        "\n",
        "- Những đặc điểm chính trong lập trình với pytorch\n",
        "\n",
        "- Những class quan trọng trong việc xây dựng mô hình Neural Network"
      ],
      "metadata": {
        "id": "rCsxG5Z6dnNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "DooVaYpkoKrW"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "drJXo-qzcaFU"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network architecture\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "OcLzrGS3oc6s"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_SGD = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer_Adam = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "9wom_M4fogjt"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train(model, optimizer, criterion, trainloader, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:    # Print every 100 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 100))\n",
        "                running_loss = 0.0\n"
      ],
      "metadata": {
        "id": "PL4mW49aoim4"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with different optimizers\n",
        "print(\"Training with SGD optimizer:\")\n",
        "train(model, optimizer_SGD, criterion, trainloader)\n",
        "\n",
        "print(\"\\nTraining with Adam optimizer:\")\n",
        "train(model, optimizer_Adam, criterion, trainloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6fvz973olJu",
        "outputId": "c18e2164-b1a6-4de1-f7a7-192306330380"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with SGD optimizer:\n",
            "[1,   100] loss: 1.004\n",
            "[1,   200] loss: 0.506\n",
            "[1,   300] loss: 0.433\n",
            "[1,   400] loss: 0.385\n",
            "[1,   500] loss: 0.349\n",
            "[1,   600] loss: 0.320\n",
            "[1,   700] loss: 0.322\n",
            "[1,   800] loss: 0.302\n",
            "[1,   900] loss: 0.293\n",
            "[2,   100] loss: 0.280\n",
            "[2,   200] loss: 0.252\n",
            "[2,   300] loss: 0.238\n",
            "[2,   400] loss: 0.233\n",
            "[2,   500] loss: 0.217\n",
            "[2,   600] loss: 0.217\n",
            "[2,   700] loss: 0.224\n",
            "[2,   800] loss: 0.215\n",
            "[2,   900] loss: 0.220\n",
            "[3,   100] loss: 0.203\n",
            "[3,   200] loss: 0.198\n",
            "[3,   300] loss: 0.184\n",
            "[3,   400] loss: 0.183\n",
            "[3,   500] loss: 0.186\n",
            "[3,   600] loss: 0.172\n",
            "[3,   700] loss: 0.170\n",
            "[3,   800] loss: 0.167\n",
            "[3,   900] loss: 0.175\n",
            "[4,   100] loss: 0.165\n",
            "[4,   200] loss: 0.156\n",
            "[4,   300] loss: 0.166\n",
            "[4,   400] loss: 0.164\n",
            "[4,   500] loss: 0.156\n",
            "[4,   600] loss: 0.158\n",
            "[4,   700] loss: 0.164\n",
            "[4,   800] loss: 0.157\n",
            "[4,   900] loss: 0.145\n",
            "[5,   100] loss: 0.136\n",
            "[5,   200] loss: 0.156\n",
            "[5,   300] loss: 0.156\n",
            "[5,   400] loss: 0.139\n",
            "[5,   500] loss: 0.135\n",
            "[5,   600] loss: 0.137\n",
            "[5,   700] loss: 0.151\n",
            "[5,   800] loss: 0.136\n",
            "[5,   900] loss: 0.129\n",
            "\n",
            "Training with Adam optimizer:\n",
            "[1,   100] loss: 0.227\n",
            "[1,   200] loss: 0.228\n",
            "[1,   300] loss: 0.226\n",
            "[1,   400] loss: 0.256\n",
            "[1,   500] loss: 0.233\n",
            "[1,   600] loss: 0.236\n",
            "[1,   700] loss: 0.256\n",
            "[1,   800] loss: 0.236\n",
            "[1,   900] loss: 0.221\n",
            "[2,   100] loss: 0.220\n",
            "[2,   200] loss: 0.206\n",
            "[2,   300] loss: 0.202\n",
            "[2,   400] loss: 0.224\n",
            "[2,   500] loss: 0.223\n",
            "[2,   600] loss: 0.215\n",
            "[2,   700] loss: 0.208\n",
            "[2,   800] loss: 0.219\n",
            "[2,   900] loss: 0.209\n",
            "[3,   100] loss: 0.190\n",
            "[3,   200] loss: 0.194\n",
            "[3,   300] loss: 0.201\n",
            "[3,   400] loss: 0.185\n",
            "[3,   500] loss: 0.189\n",
            "[3,   600] loss: 0.202\n",
            "[3,   700] loss: 0.187\n",
            "[3,   800] loss: 0.201\n",
            "[3,   900] loss: 0.181\n",
            "[4,   100] loss: 0.170\n",
            "[4,   200] loss: 0.185\n",
            "[4,   300] loss: 0.185\n",
            "[4,   400] loss: 0.180\n",
            "[4,   500] loss: 0.179\n",
            "[4,   600] loss: 0.195\n",
            "[4,   700] loss: 0.187\n",
            "[4,   800] loss: 0.184\n",
            "[4,   900] loss: 0.190\n",
            "[5,   100] loss: 0.162\n",
            "[5,   200] loss: 0.163\n",
            "[5,   300] loss: 0.179\n",
            "[5,   400] loss: 0.170\n",
            "[5,   500] loss: 0.180\n",
            "[5,   600] loss: 0.173\n",
            "[5,   700] loss: 0.172\n",
            "[5,   800] loss: 0.179\n",
            "[5,   900] loss: 0.177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Hãy trình bày về Pytorch:**\n",
        "\n",
        "**Những đặc điểm chính khi so sánh với tensorflow**"
      ],
      "metadata": {
        "id": "Xz3kX5nGovKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TensorFlow**\n",
        "\n",
        "Các mô hình TensorFlow thường được tạo ra với sự trợ giúp của Keras.\n",
        "\n",
        "Lưu ý về Keras, Đây là một thư viện mã nguồn mở chỉ cung cấp các API cấp cao, không giống như TensorFlow cung cấp cả API cấp cao và cấp thấp. Nhưng vì Keras được xây dựng bằng Python, nên việc phát triển các mô hình trong Keras thân thiện với người dùng hơn so với TensorFlow.\n",
        "\n",
        "Bất kỳ phương pháp nào trong ba cách tiếp cận đều có thể được áp dụng để phát triển các mô hình trong Keras như: Phân lớp, API chức năng và API mô hình tuần tự.\n",
        "\n",
        "+ Phân lớp: lớp tf.keras.Model có thể được sử dụng để phát triển các mô hình có thể tùy chỉnh về tính hoàn chỉnh của nó và logic chuyển tiếp được triển khai trong phương thức gọi trong khi các lớp được xác định trong phương thức _init_ (). Quan trọng nhất, cách tiếp cận hướng đối tượng giúp tái sử dụng các lớp nhiều lần và xác định chuyển tiếp cực kỳ phức tạp.\n",
        "\n",
        "+ API chức năng: đây là một cách tiếp cận rất thân thiện với người dùng để phát triển mô hình Mạng Nơ-ron so với Phân lớp theo khuyến nghị của cộng đồng các nhà phát triển. Cách tiếp cận này đòi hỏi một chút ít mã hóa vì đầu vào của lớp trước ngay lập tức được truyền vào và khi layer được xác định. Mô hình được khởi tạo thông qua các tensor đầu vào và đầu ra.\n",
        "\n",
        "+ API mô hình tuần tự: nó là một dạng rút gọn cho một mô hình có thể huấn luyện, chỉ bao gồm một vài lớp phổ biến và do đó là một cách rất nhỏ gọn và đơn gian để xác định mô hình. Mặt khác, cách tiếp cận này hoạt động rất tốt về mặt hiệu suất khi tạo ra Mạng Nơ-ron đơn giản nhưng việc triển khai Mạng Nơ-ron phức tạp trở nên rất khó khăn trong những từ khác không thể linh hoạt.\n",
        "\n",
        "\n",
        "**PyTorch**\n",
        "\n",
        "So với TensorFlow hoặc Keras, chỉ có hai cách tiếp cận để phát triển các mô hình Mạng Nơ-ron với sự trợ giúp của PyTorch: Phân lớp và tuần tự.\n",
        "\n",
        "+ Phân lớp: việc thực hiện với phương pháp này rất giống với trong TensorFlow (Keras). Ở đây, việc phân lớp được thực hiện thông qua mô đun nn.Model và các layers được xác định trong phương thức _init_ () nhưng việc tạo chuyển tiếp được thực hiện trong phương thức chuyển tiếp thay vì gọi như trong TensorFlow (Keras).\n",
        "\n",
        "Trong PyTorch cần phải có kích thước hạt nhân chính xác để làm cho nó có mức trung bình toàn cầu, lý do là chỉ có một lớp tổng hợp trung bình có sẵn.\n",
        "\n",
        "+ Tuần tự: cách tiếp cận này cũng rất giống với cách nó được thực hiện trong TensorFlow (Keras) và được thực hiện thông qua mô-đun Sequential"
      ],
      "metadata": {
        "id": "cfFSaVEfpMuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Những đặc điểm chính trong lập trình với pytorch**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b6TGnq7_pTeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Dynamic Computation Graphs: Một trong những đặc điểm chính của PyTorch là sử dụng dynamic computation graphs. Điều này có nghĩa là đồ thị tính toán có thể thay đổi trong quá trình chạy. Điều này mang lại tính linh hoạt cao cho việc xây dựng và điều chỉnh mô hình, đặc biệt là trong quá trình thử nghiệm và debug.\n",
        "\n",
        "+ Tích hợp tốt với Python: PyTorch được thiết kế để làm việc tốt với Python và có cú pháp gần gũi với ngôn ngữ này. Điều này làm cho việc xây dựng và debug mô hình trở nên dễ dàng hơn, đặc biệt là đối với những người đã quen thuộc với Python.\n",
        "\n",
        "+ Hỗ trợ GPU mạnh mẽ: PyTorch cung cấp hỗ trợ mạnh mẽ cho việc tính toán trên GPU, giúp tăng tốc quá trình huấn luyện mô hình. Việc chuyển đổi giữa tính toán trên CPU và GPU cũng được thực hiện một cách dễ dàng thông qua PyTorch.\n",
        "\n",
        "+ Các utility functions và API đầy đủ: PyTorch cung cấp một loạt các utility functions và API cho việc xử lý dữ liệu, xây dựng mô hình, và tối ưu hóa. Điều này bao gồm các lớp mạng nơ-ron, hàm loss function, optimizer và các công cụ hỗ trợ việc tiền xử lý dữ liệu.\n",
        "\n",
        "+ Sự phát triển mạnh mẽ của cộng đồng: PyTorch là một dự án mã nguồn mở với một cộng đồng lớn và tích cực phát triển. Điều này có nghĩa là có rất nhiều tài liệu học tập, mã nguồn mẫu và các thư viện bổ sung được phát triển bởi cộng đồng người dùng PyTorch."
      ],
      "metadata": {
        "id": "_f3cZ7Y5q4OY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Những class quan trọng trong việc xây dựng mô hình Neural Network**"
      ],
      "metadata": {
        "id": "sqDflWCJpYd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trong việc xây dựng mô hình Neural Network bằng PyTorch:**\n",
        "\n",
        "torch.nn.Module: Là lớp cơ sở cho tất cả các mô hình trong PyTorch. Các mô hình được xây dựng bằng cách kế thừa từ lớp này và định nghĩa phương thức forward. Trong phương thức forward, người lập trình định nghĩa cách tính toán của mô hình.\n",
        "\n",
        "torch.nn.Linear: Là lớp thực hiện phép biến đổi tuyến tính, thường được sử dụng cho các layer fully connected trong mạng nơ-ron. Lớp này chấp nhận đầu vào là một tensor có số chiều tùy ý và trả về một tensor với số chiều tương ứng.\n",
        "\n",
        "torch.nn.Conv2d và torch.nn.ConvTranspose2d: Đây là lớp convolution và deconvolution 2D. Convolution layer thực hiện phép tích chập trên dữ liệu đầu vào, trong khi deconvolution layer thực hiện phép chập ngược lại. Cả hai lớp này thường được sử dụng trong các mạng nơ-ron sâu cho việc xử lý hình ảnh.\n",
        "\n",
        "torch.nn.ReLU, torch.nn.Sigmoid, torch.nn.Tanh, etc.: Đây là các lớp activation function thường được sử dụng trong các mạng nơ-ron. Chúng được áp dụng sau mỗi layer để tạo ra đầu ra phi tuyến tính và giúp mô hình học được các mối quan hệ phức tạp giữa các đặc trưng đầu vào và đầu ra.\n",
        "\n",
        "torch.nn.BatchNorm2d và torch.nn.BatchNorm1d: Là lớp Batch Normalization dùng để ổn định và tăng tốc quá trình huấn luyện của mạng nơ-ron. Chúng giúp chuẩn hóa đầu ra của các layer trước khi được đưa vào các activation function, giảm hiện tượng vanishing gradient và tăng tốc độ hội tụ của mô hình.\n",
        "\n",
        "torch.nn.Dropout: Là lớp dropout, được sử dụng để ngẫu nhiên \"tắt\" một số đơn vị nơ-ron trong quá trình huấn luyện. Điều này giúp giảm overfitting và làm cho mô hình trở nên tổng quát hơn.\n",
        "\n",
        "torch.nn.CrossEntropyLoss, torch.nn.MSELoss, etc.: Là các hàm loss function thường được sử dụng để đo lường sự khác biệt giữa dự đoán của mô hình và giá trị thực tế của đầu ra. Các hàm này thường được sử dụng trong quá trình huấn luyện để tính toán loss và điều chỉnh các tham số của mô hình."
      ],
      "metadata": {
        "id": "yRTxQ4-9q4SD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Load data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "        self.dropout = nn.Dropout(0.5)  # Dropout layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
        "\n",
        "# Training loop\n",
        "def train(model, optimizer, criterion, train_loader, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:  # Print every 100 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
        "                running_loss = 0.0\n",
        "\n",
        "# Train model\n",
        "train(model, optimizer, criterion, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXgQQRnrsdr3",
        "outputId": "3c47fff3-a236-4764-a9de-32a7728f555f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   100] loss: 1.669\n",
            "[1,   200] loss: 0.941\n",
            "[1,   300] loss: 0.778\n",
            "[1,   400] loss: 0.695\n",
            "[1,   500] loss: 0.662\n",
            "[1,   600] loss: 0.622\n",
            "[1,   700] loss: 0.590\n",
            "[1,   800] loss: 0.566\n",
            "[1,   900] loss: 0.555\n",
            "[2,   100] loss: 0.508\n",
            "[2,   200] loss: 0.524\n",
            "[2,   300] loss: 0.520\n",
            "[2,   400] loss: 0.506\n",
            "[2,   500] loss: 0.504\n",
            "[2,   600] loss: 0.459\n",
            "[2,   700] loss: 0.449\n",
            "[2,   800] loss: 0.470\n",
            "[2,   900] loss: 0.463\n",
            "[3,   100] loss: 0.450\n",
            "[3,   200] loss: 0.461\n",
            "[3,   300] loss: 0.454\n",
            "[3,   400] loss: 0.422\n",
            "[3,   500] loss: 0.425\n",
            "[3,   600] loss: 0.436\n",
            "[3,   700] loss: 0.407\n",
            "[3,   800] loss: 0.430\n",
            "[3,   900] loss: 0.453\n",
            "[4,   100] loss: 0.434\n",
            "[4,   200] loss: 0.408\n",
            "[4,   300] loss: 0.437\n",
            "[4,   400] loss: 0.407\n",
            "[4,   500] loss: 0.402\n",
            "[4,   600] loss: 0.404\n",
            "[4,   700] loss: 0.400\n",
            "[4,   800] loss: 0.411\n",
            "[4,   900] loss: 0.380\n",
            "[5,   100] loss: 0.396\n",
            "[5,   200] loss: 0.394\n",
            "[5,   300] loss: 0.390\n",
            "[5,   400] loss: 0.398\n",
            "[5,   500] loss: 0.376\n",
            "[5,   600] loss: 0.397\n",
            "[5,   700] loss: 0.399\n",
            "[5,   800] loss: 0.410\n",
            "[5,   900] loss: 0.384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trong việc xây dựng mô hình Neural Network bằng TensorFlow:**\n",
        "\n",
        "tf.keras.layers.Dense: Lớp này thực hiện phép biến đổi tuyến tính, thường được sử dụng cho các layer fully connected trong mạng nơ-ron. Đầu vào và đầu ra của nó là các tensor có số chiều tùy ý.\n",
        "\n",
        "tf.keras.layers.Conv2D và tf.keras.layers.Conv2DTranspose: Đây là lớp convolution và deconvolution 2D. Lớp convolution thực hiện phép tích chập trên dữ liệu đầu vào, trong khi lớp deconvolution thực hiện phép chập ngược lại. Cả hai lớp này thường được sử dụng trong các mạng nơ-ron sâu cho việc xử lý hình ảnh.\n",
        "\n",
        "tf.keras.layers.Activation: Lớp này áp dụng một hàm activation function lên đầu ra của các layer trước đó. Các activation function thông thường bao gồm ReLU, Sigmoid, Tanh, ...\n",
        "\n",
        "tf.keras.layers.Dropout: Lớp này thực hiện phép dropout, ngẫu nhiên \"tắt\" một số đơn vị nơ-ron trong quá trình huấn luyện. Điều này giúp giảm overfitting và làm cho mô hình trở nên tổng quát hơn.\n",
        "\n",
        "tf.keras.layers.BatchNormalization: Lớp này thực hiện batch normalization, giúp ổn định và tăng tốc quá trình huấn luyện của mạng nơ-ron. Chúng chuẩn hóa đầu ra của các layer trước khi được đưa vào các activation function, giảm hiện tượng vanishing gradient và tăng tốc độ hội tụ của mô hình.\n",
        "\n",
        "tf.keras.losses: Module này chứa nhiều hàm loss function thường được sử dụng, như Mean Squared Error (MSE), Categorical Crossentropy, Binary Crossentropy, ...\n",
        "\n",
        "tf.keras.optimizers: Module này chứa các optimizer thường được sử dụng cho việc tối ưu hóa mạng nơ-ron, bao gồm SGD, Adam, RMSprop, ...\n"
      ],
      "metadata": {
        "id": "l3srdAhvsJKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load data\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Build model\n",
        "model = models.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28)),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "# Evaluate model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evz6mrkRsbJ3",
        "outputId": "be6078eb-644f-4a90-e877-4599c40dbda9"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 9s 4ms/step - loss: 0.2972 - accuracy: 0.9126\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1414 - accuracy: 0.9572\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1082 - accuracy: 0.9678\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0874 - accuracy: 0.9725\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0732 - accuracy: 0.9771\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0762 - accuracy: 0.9775\n",
            "Test accuracy: 0.9775000214576721\n"
          ]
        }
      ]
    }
  ]
}